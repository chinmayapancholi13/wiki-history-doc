{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import bz2\n",
    "import urllib2\n",
    "\n",
    "BZ2_CHUNK = 10*1000*1024\n",
    "STR_CHUNK = 400*1000*1024\n",
    "\n",
    "#url = 'http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles26.xml-p42567204p42663461.bz2'\n",
    "#url = 'http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-meta-current1.xml-p10p30303.bz2'\n",
    "\n",
    "sys.argv = [\"\",\"\"]\n",
    "sys.argv[1] = 'enwiki-latest-pages-articles26.xml-p42567204p42663461.bz2'\n",
    "#sys.argv[1] = 'extracted/AA/wiki_00.bz2'\n",
    "#sys.argv[1] = 'enwiki-latest-pages-meta-current1.xml-p10p30303.bz2'\n",
    "#sys.argv[1] = 'http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-meta-current1.xml-p10p30303.bz2'\n",
    "if len(sys.argv) > 1:\n",
    "    url = sys.argv[1]\n",
    "\n",
    "http = False\n",
    "if len(url.split('://')) > 1:\n",
    "    http = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def keepText(text):\n",
    "    start = 0\n",
    "    lst = text.split(\"==References==\")\n",
    "    if len(lst) == 1:\n",
    "        return \"\" \n",
    "    text = lst[0]\n",
    "    lst1 = [m.end(0) for m in re.finditer(\"}}\\n\", text)]\n",
    "    if len(lst1) > 1:\n",
    "        return text[lst1[-1]:]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "#parse one page using split(). Fast!\n",
    "def parser(buf):\n",
    "    keys_h_t = {'title': ('<title>', '</title>'), \n",
    "        'redirect': ('<redirect ', ' />'),\n",
    "        'id': ('<id>', '</id>'), \n",
    "        'text': ('<text ', '</text>')}\n",
    "    d = {k: '' for k in keys_h_t.keys()}\n",
    "    for key, (head, tail) in keys_h_t.items():\n",
    "        #get rid of front\n",
    "        lst = buf.split(head)\n",
    "        if len(lst) > 1:\n",
    "            #get rid of tail\n",
    "            lst = lst[1].strip().split(tail)\n",
    "            if len(lst) > 1:\n",
    "                d[key] = lst[0]\n",
    "#    if d['id'] == '42630050':\n",
    "#        print  d['text']\n",
    "    d['text'] = keepText(d['text'])\n",
    "    if d['redirect'] != '':\n",
    "        d['text'] = '<redirect to ' + d['redirect'] + ' />'\n",
    "#    if d['id'] == '42630050':\n",
    "#        print \"*****************************\", start, d['text']    \n",
    "    return d\n",
    "\n",
    "#parse one page using regex. Slow!\n",
    "import re\n",
    "regex = [\"<title>[\\S\\s]+?</title>\", \"<id>[\\S\\s]+?</id>\", \"<ns>[\\S\\s]+?</ns>\",\"<text[\\S\\s]+?</text>\"]\n",
    "def parser1(buf):\n",
    "    keys = ['title',  'id', 'ns', 'text']\n",
    "    d = {k: '' for k in keys}\n",
    "    for i, k in enumerate(keys): \n",
    "        #print i,k, regex[i], buf[:100]\n",
    "        lst = re.findall(regex[i], buf, re.IGNORECASE)\n",
    "        if len(lst) > 0:\n",
    "            d[k] = lst[0]\n",
    "    for k in keys: \n",
    "        len1 = len(k) + 2\n",
    "        d[k]=d[k][len1:-len1-1]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##single thread\n",
    "\n",
    "#support getPagesByUrl() and getPagesByPath()\n",
    "def getPages(chunk):\n",
    "    lst = chunk.split(\"<page>\")\n",
    "    #pages = map(lambda page: parser1(page), lst)\n",
    "    pages = map(lambda page: parser(page), lst)  ############################################\n",
    "    return pages\n",
    "\n",
    "#parse page with url input\n",
    "def getPagesByUrl(url, bytes = BZ2_CHUNK):\n",
    "\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    req = urllib2.urlopen(url)\n",
    "    pages = []\n",
    "    b = 0\n",
    "    while True:\n",
    "        t1 = time.time()\n",
    "        chunk = req.read(bytes)\n",
    "        if not chunk:\n",
    "            break\n",
    "        text = decompressor.decompress(chunk)\n",
    "        pages += getPages(text)\n",
    "        b += len(text)\n",
    "        print \"%.6f\"%(time.time()-t1) ,len(pages), b\n",
    "    req.close()\n",
    "\n",
    "    return pages, b\n",
    "\n",
    "#parse page with file_path input\n",
    "def getPagesByPath(path, bytes = STR_CHUNK):\n",
    "\n",
    "    fd = bz2.BZ2File(path, 'rb')\n",
    "    pages = []\n",
    "    b = 0\n",
    "    raw = \"\"   ####################    \n",
    "    while True:\n",
    "        t1 = time.time()\n",
    "        chunk = fd.read(bytes)\n",
    "        if not chunk:\n",
    "            break\n",
    "        pages += getPages(chunk)\n",
    "        b += len(chunk)\n",
    "        print \"%.6f\"%(time.time()-t1) ,len(pages), b\n",
    "        raw += chunk ################\n",
    "    fd.close()\n",
    "\n",
    "    return pages, b, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#multithreading parsing\n",
    "import threading\n",
    "import Queue\n",
    "def threadwork(chunk, out_queue):\n",
    "    lst = chunk.split(\"<page>\")\n",
    "    #pages = map(lambda page: parser1(page), lst)   #############################################\n",
    "    pages = map(lambda page: parser(page), lst)\n",
    "    return out_queue.put(pages )\n",
    "\n",
    "#multithreading parsing pages with a file_path input\n",
    "def getPagesByPath_multithread(path, bytes = STR_CHUNK):\n",
    "    t0 = time.time()\n",
    "    my_queue = Queue.Queue()\n",
    "    thread_list = []\n",
    "    n_thread = 0\n",
    "\n",
    "    fd = bz2.BZ2File(path, 'rb')\n",
    "    b = 0\n",
    "    raw = \"\"   ####################\n",
    "    while True:\n",
    "        chunk = fd.read(bytes)\n",
    "        if not chunk:\n",
    "            break\n",
    "        t = threading.Thread(target=threadwork, args=(chunk ,my_queue))\n",
    "        thread_list.append(t)\n",
    "        b += len(chunk)\n",
    "        print \"thread: %d start time: %.6f, buf: %d\"%(n_thread, time.time()-t0 ,b)\n",
    "        n_thread += 1\n",
    "        t.start()\n",
    "        raw += chunk ################\n",
    "    fd.close()\n",
    "    for t in thread_list:\n",
    "        t.join()\n",
    "    pages = []\n",
    "    for i in xrange(n_thread):\n",
    "        pages += my_queue.get()\n",
    "        print \"thread: %d done time: %.6f, page_len: %d\"%(i, time.time()-t0 ,len(pages))\n",
    "\n",
    "    return pages, b, raw   ##################\n",
    "\n",
    "#multithreading parsing pages with an url input\n",
    "def getPagesByUrl_multithread(url, bytes = BZ2_CHUNK):\n",
    "    t0 = time.time()\n",
    "    my_queue = Queue.Queue()\n",
    "    thread_list = []\n",
    "    n_thread = 0\n",
    "\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    req = urllib2.urlopen(url)\n",
    "    b = 0\n",
    "    while True:\n",
    "        chunk = req.read(bytes)\n",
    "        if not chunk:\n",
    "            break\n",
    "        text = decompressor.decompress(chunk)\n",
    "        t = threading.Thread(target=threadwork, args=(text ,my_queue))\n",
    "        thread_list.append(t)\n",
    "        b += len(text)\n",
    "        print \"thread: %d start time: %.6f, buf: %d\"%(n_thread, time.time()-t0 ,b)\n",
    "        n_thread += 1\n",
    "        t.start()\n",
    "    req.close()\n",
    "    for t in thread_list:\n",
    "        t.join()\n",
    "    pages = []\n",
    "    for i in xrange(n_thread):\n",
    "        pages += my_queue.get()\n",
    "        print \"thread: %d done time: %.6f, page_len: %d\"%(i, time.time()-t0 ,len(pages))\n",
    "\n",
    "    return pages, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.465536 29327 96847211\n",
      "file_size: 20772450\n",
      "{'redirect': '', 'text': '', 'id': '', 'title': ''}\n",
      "('redirect', '')\n",
      "('text', \"'''Marko Virtanen''' (born December 10, 1968) is a [[Finland|Finnish]] former professional [[ice hockey]] player. He is currently the head coach of [[JYP Jyv\\xc3\\xa4skyl\\xc3\\xa4]] in the Finnish [[Liiga]].\\n\\nVirtanen assumed the position of head coach for JYP with the [[2013\\xe2\\x80\\x9314 Liiga season]].&lt;ref name=&quot;jypliiga&quot;&gt;{{cite web | url=http://www.jypliiga.fi/uutiset/marko-virtanen-jypin-paavalmentajaksi-2013-2015 | title=Marko Virtanen JYPin p\\xc3\\xa4\\xc3\\xa4valmentajaksi 2013 \\xe2\\x80\\x93 2015 | publisher=Jypliiga | accessdate=25 July 2014}}&lt;/ref&gt;\\n\\n\")\n",
      "('id', '42567219')\n",
      "('title', 'Marko Virtanen')\n",
      "total time: 2.467245 29327 96847211 96847211\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "if http:\n",
    "    #reading data from url\n",
    "    #pages, b = getPagesByUrl(url)\n",
    "    pages, b = getPagesByUrl_multithread(url)\n",
    "else:\n",
    "    #reading data from local\n",
    "    pages, b, raw = getPagesByPath(url)  \n",
    "    #pages, b, raw = getPagesByPath_multithread(url)    ###################################\n",
    "    len1 = os.path.getsize(url)\n",
    "    print \"file_size:\",len1\n",
    "\n",
    "print pages[0]\n",
    "for kv in pages[3].items():\n",
    "    print kv\n",
    "\n",
    "print \"total time: %.6f\"%(time.time()-t0), len(pages), b, len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'redirect': 'title=\"Kim Hyeon-woo\"', 'text': '<redirect to title=\"Kim Hyeon-woo\" />', 'id': '42567205', 'title': 'Kim Hyeon-Woo'}\n",
      "{'redirect': 'title=\"Lee Se-yeol\"', 'text': '<redirect to title=\"Lee Se-yeol\" />', 'id': '42567208', 'title': 'Lee Se-Yeol'}\n"
     ]
    }
   ],
   "source": [
    "print pages[1]\n",
    "print pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2page = {}\n",
    "for i, p in enumerate(pages):\n",
    "    p['pos'] = i\n",
    "    id2page[p['id']]= p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17684\n"
     ]
    }
   ],
   "source": [
    "pages_filtered = filter(lambda p: p['text'] != '', pages)\n",
    "print len(pages_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42610148 Mahoş River \n",
      "<redirect to title=\"Mahoș River\" />\n",
      "42618301 8th Observation Squadron \n",
      "<redirect to title=\"914th Expeditionary Air Refueling Squadron\" />\n",
      "42615649 Tortyra contubernalis \n",
      "<redirect to title=\"Ornarantia contubernalis\" />\n"
     ]
    }
   ],
   "source": [
    "id2page_filtered = {p['id']: p for p in pages_filtered}\n",
    "for k,v in id2page_filtered.items()[:3]:\n",
    "    print k, v['title'],\"\\n\",v['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29327 29327 17684\n"
     ]
    }
   ],
   "source": [
    "#print id2page['42630050']\n",
    "raw_lst = raw.split(\"<page>\")\n",
    "print len(pages), len(raw_lst), len(pages_filtered)\n",
    "#print [raw_lst[19438]], \"\\n*******************\\n\",pages[19438]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <title>Kim Hyeon-Woo</title>\n",
      "    <ns>0</ns>\n",
      "    <id>42567205</id>\n",
      "    <redirect title=\"Kim Hyeon-woo\" />\n",
      "    <revision>\n",
      "      <id>605512342</id>\n",
      "      <timestamp>2014-04-23T21:02:00Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Mohsen1248</username>\n",
      "        <id>3761856</id>\n",
      "      </contributor>\n",
      "      <comment>Mohsen1248 moved page [[Kim Hyeon-Woo]] to [[Kim Hyeon-woo]]</comment>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Kim Hyeon-woo]]\n",
      "{{R from move}}</text>\n",
      "      <sha1>16kop2unyres98bcaxkfj1674q7xcfd</sha1>\n",
      "    </revision>\n",
      "  </page>\n",
      "  \n",
      "\n",
      "    <title>Lee Se-Yeol</title>\n",
      "    <ns>0</ns>\n",
      "    <id>42567208</id>\n",
      "    <redirect title=\"Lee Se-yeol\" />\n",
      "    <revision>\n",
      "      <id>605512408</id>\n",
      "      <timestamp>2014-04-23T21:02:32Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Mohsen1248</username>\n",
      "        <id>3761856</id>\n",
      "      </contributor>\n",
      "      <comment>Mohsen1248 moved page [[Lee Se-Yeol]] to [[Lee Se-yeol]]</comment>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text xml:space=\"preserve\">#REDIRECT [[Lee Se-yeol]]\n",
      "{{R from move}}</text>\n",
      "      <sha1>6d40t9mkghnknpebayjf6uhmgswkp0r</sha1>\n",
      "    </revision>\n",
      "  </page>\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print raw_lst[1]\n",
    "print raw_lst[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from bz2 time: 71.596046 16668087\n",
      "<doc id=\"42567219\" url=\"https://en.wikipedia.org/wiki?curid=42567219\" title=\"Marko Virtanen\">\n",
      "Marko Virtanen\n",
      "\n",
      "Marko Virtanen (born December 10, 1968) is a Finnish former professional ice hockey player. He is currently the head coach of JYP Jyväskylä in the Finnish Liiga.\n",
      "Virtanen assumed the position of head coach for JYP with the 2013–14 Liiga season.\n",
      "\n",
      "</doc>\n",
      "<doc id=\"42567222\" url=\"https://en.wikipedia.org/wiki?curid=42567222\" title=\"File:David and Goliath -1700s.jpg\">\n",
      "File:David and Goliath -1700s.jpg\n",
      "\n",
      "\n",
      "</doc>\n",
      "<doc id=\"42567223\" url=\"https://en.wikipedia.org/wiki?curid=42567223\" title=\"2014–15 Northern Illinois Huskies men's basketball team\">\n",
      "2014–15 Northern Illinois Huskies men's basketball team\n",
      "\n",
      "The 2014–15 Northern Illinois Huskies men's basketball team represented Northern Illinois University during the 2014–15 NCAA Division I men's basketball season. The Huskies, led by fourth year head coach Mark Montgomery, played their home games at the Convocation Center as mem\n"
     ]
    }
   ],
   "source": [
    "fd = bz2.BZ2File('extracted/AA/wiki_00.bz2', 'rb')\n",
    "uncompressed = fd.read()\n",
    "print \"reading from bz2 time: %.6f\"%(time.time()-t0) ,len(uncompressed)\n",
    "fd.close()\n",
    "print uncompressed[:1000]\n",
    "\n",
    "def parsePage(buf):\n",
    "    keys_h_t = {'title': ('title=\"', '\">'), \n",
    "        'ns': ('<ns>', '</ns>'),\n",
    "        'id': ('id=\"', '\" url='), \n",
    "        'text': ('\">', '</doc>')}\n",
    "    d = {k: '' for k in keys_h_t.keys()}\n",
    "    for key, (head, tail) in keys_h_t.items():\n",
    "        #get rid of front\n",
    "        lst = buf.split(head)\n",
    "        if len(lst) > 1:\n",
    "            #get rid of tail\n",
    "            lst = lst[1].strip().split(tail)\n",
    "            if len(lst)> 1:\n",
    "                d[key] = lst[0].strip()\n",
    "    return d\n",
    "        \n",
    "\n",
    "pages1 = uncompressed.split(\"<doc \")\n",
    "pages1 = map(lambda buf: parsePage(buf), pages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14946\n",
      "42662895 Thomas Ignatius McCarthy \n",
      "Thomas Ignatius McCarthy\n",
      "\n",
      "Thomas Ignatius McCarthy, LRIBA (born 31 January 1880, died 13 Feb 1951) was an architect based in Coalville, Leicestershire.\n",
      "Early in the twentieth century, Thomas Ignatius McCarthy set up a practice as a surveyor and architect in Coalville, which was a partnership shared with Henry Collings (1880 - 1960). Collings was responsible for the design of the Coalville Clock Tower war memorial - a building admired by Pevsner.\n",
      "Examples of work by Thomas Ignatius McCarthy (some possibly in conjunction with Henry Collings):\n",
      "42615641 Category:Bronx building and structure stubs \n",
      "Category:Bronx building and structure stubs\n"
     ]
    }
   ],
   "source": [
    "pages1 = filter(lambda p: p['id'] != '', pages1)\n",
    "print len(pages1)\n",
    "id2page1 = {p['id']: p for p in pages1}\n",
    "for k,v in id2page1.items()[:2]:\n",
    "    print k, v['title'],\"\\n\",v['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42567205', '42567208', '42567219', '42567221', '42567223', '42567232', '42567236', '42567240', '42567245', '42567249', '42567253', '42567255', '42567257', '42567261', '42567270', '42567275', '42567280', '42567282', '42567283', '42567286']\n",
      "['42567219', '42567222', '42567223', '42567226', '42567244', '42567248', '42567249', '42567253', '42567257', '42567261', '42567286', '42567287', '42567292', '42567295', '42567306', '42567319', '42567320', '42567337', '42567340', '42567343']\n"
     ]
    }
   ],
   "source": [
    "lst_filtered = sorted(id2page_filtered.keys())\n",
    "lst1 = sorted(id2page1.keys())\n",
    "print lst_filtered[:20]\n",
    "print lst1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**bz3  ****\n",
      "\n",
      "\n",
      "**raw  ****\n",
      "\n",
      "    <title>Category:Airdrieonians F.C. (1878) wartime guest players</title>\n",
      "    <ns>14</ns>\n",
      "    <id>42567343</id>\n",
      "    <revision>\n",
      "      <id>605514597</id>\n",
      "      <timestamp>2014-04-23T21:17:57Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Jmorrison230582</username>\n",
      "        <id>1894081</id>\n",
      "      </contributor>\n",
      "      <comment>[[WP:AES|←]]Created page with 'Players who guested during wartime matches for [[Airdrieonians F.C. (1878)|Airdrieonians]].  [[Category:Airdrieonians F.C. (1878) players| ]] Category:Scottish...'</comment>\n",
      "      <model>wikitext</model>\n",
      "      <format>text/x-wiki</format>\n",
      "      <text xml:space=\"preserve\">Players who guested during wartime matches for [[Airdrieonians F.C. (1878)|Airdrieonians]].\n",
      "\n",
      "[[Category:Airdrieonians F.C. (1878) players| ]]\n",
      "[[Category:Scottish Football League wartime guest players by club|Airdrie]]</text>\n",
      "      <sha1>b7w1a86ggde42ddbj2msr1f1e2jkqld</sha1>\n",
      "    </revision>\n",
      "  </page>\n",
      "  \n",
      "\n",
      "**wikiex***\n",
      "Category:Airdrieonians F.C. (1878) wartime guest players\n",
      "\n",
      "Players who guested during wartime matches for Airdrieonians.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx='42567222'\n",
    "idx='42567226'\n",
    "idx='42567244'\n",
    "idx='42567248'\n",
    "idx='42567249'\n",
    "idx='42567287'\n",
    "idx='42567292'\n",
    "idx='42567306'\n",
    "idx='42567320'\n",
    "idx='42567337'\n",
    "idx='42567343'\n",
    "print \"**bz3  ****\\n\",id2page[idx]['text']\n",
    "print\n",
    "print \"**raw  ****\\n\",raw_lst[id2page[idx]['pos']]\n",
    "print\n",
    "print \"**wikiex***\\n\",id2page1[idx]['text']\n",
    "print \n",
    "#print id2page3[idx]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycurl\n",
    "from io import BytesIO\n",
    "\n",
    "buf = BytesIO()\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Wikipedia:I just don't like it\"\n",
    "url = \"https://en.wikipedia.org/?curid=42630050\"\n",
    "t0 = time.time()\n",
    "with open('out.bz2', 'wb') as f: \n",
    "    c = pycurl.Curl()\n",
    "    #c.setopt(c.URL, 'http://news.ycombinator.com')\n",
    "    #c.setopt(c.URL, 'www.gutenberg.org/ebooks/4300')\n",
    "    #c.setopt(c.URL, 'https://it.wikipedia.org/wiki/Armonium')\n",
    "    #c.setopt(c.URL, \"https://en.wikipedia.org/wiki/Wikipedia:I_just_don't_like_it\")\n",
    "    c.setopt(c.URL, url)    \n",
    "    #c.setopt(c.WRITEDATA, f)\n",
    "    c.setopt(c.WRITEDATA, buf)\n",
    "    c.perform()\n",
    "print \"downloading time: %.6f\"%(time.time()-t0)\n",
    "body =  buf.getvalue()\n",
    "buf.close()\n",
    "\n",
    "print body\n",
    "print body.decode('iso-8859-1')\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "input_file = bz2.BZ2File('out.bz2', 'rb')\n",
    "uncompressed = input_file.read()\n",
    "\n",
    "print \"reading from bz2 time: %.6f\"%(time.time()-t0) ,len(uncompressed),uncompressed\n",
    "input_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t0 = time.time()\n",
    "with open('out.bz2', 'wb') as f: \n",
    "    c = pycurl.Curl()\n",
    "    #c.setopt(c.URL, 'http://news.ycombinator.com')\n",
    "    #c.setopt(c.URL, 'www.gutenberg.org/ebooks/4300')\n",
    "    c.setopt(c.URL, 'https://it.wikipedia.org/wiki/Armonium')\n",
    "    c.setopt(c.WRITEDATA, f)\n",
    "    c.perform()\n",
    "    buf.close()\n",
    "print \"downloading time: %.6f\"%(time.time()-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "input_file = bz2.BZ2File(fname, 'rb')\n",
    "uncompressed = input_file.read()\n",
    "input_file.close()\n",
    "print \"reading from bz2 time: %.6f\"%(time.time()-t0) ,len(uncompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import bz2\n",
    "import urllib2\n",
    "url = 'http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles26.xml-p42567204p42663461.bz2'\n",
    "#url = 'http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-meta-current1.xml-p10p30303.bz2'\n",
    "fname = url.split('/')[-1]\n",
    "len1 = os.path.getsize(fname)\n",
    "print \"size:\",len1\n",
    "\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "os.system(\"wget \" + url)\n",
    "print \"downloading time: %.6f\"%(time.time()-t0)\n",
    "#\"\"\" #164 sec\n",
    "\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "input_file = bz2.BZ2File(fname, 'rb')\n",
    "uncompressed = input_file.read()\n",
    "input_file.close()\n",
    "print \"reading from bz2 time: %.6f\"%(time.time()-t0) ,len(uncompressed)\n",
    "#\"\"\" #16 sec\n",
    "\n",
    "n_bytes = 100*1000*1024\n",
    "t0 = time.time()\n",
    "decompressor = bz2.BZ2Decompressor()\n",
    "req = urllib2.urlopen(url)\n",
    "text=\"\"\n",
    "while True:\n",
    "    t1 = time.time()\n",
    "    chunk = req.read(n_bytes)\n",
    "    if not chunk:\n",
    "        break\n",
    "    decompressed = decompressor.decompress(chunk)\n",
    "    text += decompressed\n",
    "    print \"%.6f\"%(time.time()-t1) ,len(chunk), len(decompressed)\n",
    "req.close()\n",
    "print \"reading and decompressing from url time: %.6f\"%(time.time()-t0) ,len(chunk), len(decompressed)\n",
    "\n",
    "print len(text),\"\\n\",text[-1000:]\n",
    "#t0 = time.time()\n",
    "#with open(\"tmp.txt\", \"w\") as fw:\n",
    "#    fw.write(buf)\n",
    "#print \"writing time: %.6f\"%(time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(text),\"\\n\",text[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "lst =  text.split(\"<page>\")\n",
    "print \"split pages time: %.6f\"%(time.time()-t0) ,len(lst), len(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lst[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extracting header and text from each page\n",
    "import re\n",
    "regex = u\"<text xml:[\\S\\s]+?==References==\"\n",
    "\n",
    "header_lst = map(lambda page: (page[:500], re.findall(regex, page, re.IGNORECASE)), lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print header_lst[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parsing header and text\n",
    "reg_title = u\"<title>[\\S\\s]+?</title>\"\n",
    "reg_idx = u\"<id>[\\S\\s]+?</id>\"\n",
    "def fmap(header, lst):\n",
    "    text = \"\"\n",
    "    if len(lst) > 0:\n",
    "        text = lst[0]\n",
    "        \n",
    "    title_lst = re.findall(reg_title, header, re.IGNORECASE)\n",
    "    title = \"\"\n",
    "    if len(title_lst) > 0:\n",
    "        title = title_lst[0][7:-8] \n",
    "        \n",
    "    idx_lst = re.findall(reg_idx, header, re.IGNORECASE)\n",
    "    idx = \"\"\n",
    "    if len(idx_lst) > 0:\n",
    "        idx = idx_lst[0][4:-5]        \n",
    "        \n",
    "    #TODO other info: eg. id, links, tokenizing, selection .....\n",
    "    \n",
    "    return (title, idx, text)\n",
    "\n",
    "map_tuples = map(lambda (head, lst):  fmap(head, lst), header_lst)\n",
    "print len(map_tuples), len(map_tuples[0]), len(map_tuples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map_tuples[0]\n",
    "print map_tuples[1]\n",
    "print map_tuples[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
